{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c008dec8",
   "metadata": {},
   "source": [
    "# PYTHON"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44973ac0",
   "metadata": {},
   "source": [
    "Answer NO 1:-\n",
    "\n",
    " output = {\n",
    "    \"abc\": [\"def\", \"ghi\", \"jkl\", \"mno\", \"pqr\", \"stu\", \"vwx\", \"yz\"],\n",
    "    \"def\": [\"ghi\", \"jkl\", \"mno\", \"pqr\", \"stu\", \"vwx\", \"yz\"],\n",
    "    \"ghi\": [\"jkl\", \"mno\", \"pqr\", \"stu\", \"vwx\", \"yz\"],\n",
    "    \"jkl\": [\"mno\", \"pqr\", \"stu\", \"vwx\", \"yz\"],\n",
    "    \"mno\": [\"pqr\", \"stu\", \"vwx\", \"yz\"],\n",
    "    \"pqr\": [\"stu\", \"vwx\", \"yz\"],\n",
    "    \"stu\": [\"vwx\", \"yz\"],\n",
    "    \"vwx\": [\"yz\"],\n",
    "    \"yz\": [\"you are finally here !!!\"]\n",
    "}\n",
    "\n",
    "for key, value in output.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acc0363c",
   "metadata": {},
   "source": [
    "ANSWER NO 2:-\n",
    "\n",
    " \n",
    " def count_horses(stalls, distance):\n",
    "    count = 1\n",
    "    last_stall = stalls[0]\n",
    "    for stall in stalls:\n",
    "        if stall - last_stall >= distance:\n",
    "            count += 1\n",
    "            last_stall = stall\n",
    "    return count\n",
    "\n",
    "def max_min_distance(stalls, k):\n",
    "    stalls.sort()\n",
    "    left, right = 1, stalls[-1] - stalls[0] + 1\n",
    "\n",
    "    while left < right:\n",
    "        mid = left + (right - left) // 2\n",
    "        if count_horses(stalls, mid) >= k:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid\n",
    "\n",
    "    return left - 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c340f92",
   "metadata": {},
   "source": [
    " ANSWER NO 3:-"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f400592",
   "metadata": {},
   "source": [
    "ANSWER NO 4 :-def fourSum(nums, target):\n",
    "    nums.sort()\n",
    "    quadruplets = []\n",
    "    n = len(nums)\n",
    "\n",
    "    for i in range(n - 3):\n",
    "        if i > 0 and nums[i] == nums[i - 1]:\n",
    "            continue\n",
    "\n",
    "        for j in range(i + 1, n - 2):\n",
    "            if j > i + 1 and nums[j] == nums[j - 1]:\n",
    "                continue\n",
    "\n",
    "            left = j + 1\n",
    "            right = n - 1\n",
    "\n",
    "            while left < right:\n",
    "                current_sum = nums[i] + nums[j] + nums[left] + nums[right]\n",
    "\n",
    "                if current_sum == target:\n",
    "                    quadruplets.append([nums[i], nums[j], nums[left], nums[right]])\n",
    "\n",
    "                    while left < right and nums[left] == nums[left + 1]:\n",
    "                        left += 1\n",
    "                    while left < right and nums[right] == nums[right - 1]:\n",
    "                        right -= 1\n",
    "\n",
    "                    left += 1\n",
    "                    right -= 1\n",
    "                elif current_sum < target:\n",
    "                    left += 1\n",
    "                else:\n",
    "                    right -= 1\n",
    "\n",
    "    return quadruplets\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7a130",
   "metadata": {},
   "source": [
    "# SQL\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04334539",
   "metadata": {},
   "source": [
    "ANSWER NO 1:-\n",
    "\n",
    "To fix this issue, assuming you intend to select all the runners who have not won any races, you can rewrite the query using a LEFT JOIN to handle NULL values explicitly:\n",
    "\n",
    "SELECT r.*\n",
    "FROM runners r\n",
    "LEFT JOIN races ra ON r.id = ra.winner_id\n",
    "WHERE ra.winner_id IS NULL OR ra.winner_id = '';\n",
    "\n",
    "This approach explicitly handles NULL values and empty strings, providing a more reliable way to achieve the desired result of selecting runners who have not won any races."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6771c23d",
   "metadata": {},
   "source": [
    "ANSWER NO 2:-\n",
    " \n",
    " To fetch values from table test_a that are not in test_b without using the NOT keyword, you can use a LEFT JOIN and filter out the rows where there is no match in test_b. Here's it is:\n",
    " \n",
    " SELECT a.id\n",
    "FROM test_a a\n",
    "LEFT JOIN test_b b ON a.id = b.id\n",
    "WHERE b.id IS NULL;\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d256824",
   "metadata": {},
   "source": [
    " ANSWER NO 3:-\n",
    "\n",
    " SELECTA\n",
    "    u.user_id,\n",
    "    u.username,\n",
    "    td.training_id,\n",
    "    COUNT(*) AS times_taken\n",
    "FROM \n",
    "    users u\n",
    "JOIN \n",
    "    training_details td ON u.user_id = td.user_id\n",
    "GROUP BY \n",
    "    u.user_id, td.training_id, td.training_date\n",
    "HAVING \n",
    "    COUNT(*) > 1\n",
    "ORDER BY \n",
    "    td.training_date DESC;\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bafb52e6",
   "metadata": {},
   "source": [
    "ANSWER NO 4:-\n",
    "    \n",
    " SELECT Manager_Id AS Manager_d, \n",
    "       Emp_name AS Manager, \n",
    "       AVG(Salary) AS Average_Salary_Under_Manager\n",
    "FROM Employees\n",
    "WHERE Manager_Id IS NOT NULL\n",
    "GROUP BY Manager_Id, Emp_name\n",
    "ORDER BY Manager_Id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a62bb",
   "metadata": {},
   "source": [
    "# STATISTICS "
   ]
  },
  {
   "cell_type": "raw",
   "id": "be2d218d",
   "metadata": {},
   "source": [
    "ANSWER NO 1:-\n",
    "\n",
    "Six Sigma is a statistical concept that originated in the manufacturing industry and is used to measure and improve the quality of processes by minimizing defects and variations. The term \"Six Sigma\" refers to a process that operates with extremely high accuracy and precision, with only 3.4 defects per million opportunities.\n",
    "\n",
    "In statistical terms, the Sigma (Ïƒ) symbol represents the standard deviation, which measures the amount of variation or dispersion in a set of data. In a Six Sigma process, the goal is to reduce this variation so that the process operates consistently and produces high-quality output.\n",
    "\n",
    "Here's how Six Sigma is typically applied statistically:\n",
    "\n",
    "Defining Defects:  \n",
    "\n",
    "Measuring Defects:  \n",
    "\n",
    "Calculating Sigma Level:  \n",
    "\n",
    "Example:  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7a01157",
   "metadata": {},
   "source": [
    "ANSWER NO 2:-\n",
    "    \n",
    "Data that do not adhere to a log-normal or Gaussian distribution can be found across various domains. Here are some examples:\n",
    "\n",
    "Internet Traffic: The volume of data transferred over the internet often follows a distribution that is far from Gaussian. Internet traffic tends to have bursts and spikes during certain periods, leading to a distribution that is skewed and potentially heavy-tailed.\n",
    "\n",
    "Income Distribution: The distribution of incomes within a population is typically highly skewed, with a small percentage of individuals earning a disproportionately large share of the total income. This distribution does not conform to a Gaussian or log-normal pattern, as it exhibits a long tail on the higher income side.\n",
    "\n",
    "Power Grid Usage: The usage of electricity in a power grid can exhibit non-Gaussian behavior. During peak hours, there may be significant spikes in electricity consumption, leading to a distribution that is skewed and potentially has heavy tails.\n",
    "\n",
    "Customer Purchase Behavior: The amount of money spent by customers in retail stores or online shops can vary widely and often does not follow a Gaussian distribution. There may be a small number of customers making large purchases, while the majority make smaller purchases, resulting in a skewed distribution.\n",
    "\n",
    "Earthquake Magnitudes: The magnitudes of earthquakes follow a distribution known as the Gutenberg-Richter law, which is characterized by a power-law relationship rather than a Gaussian or log-normal distribution. This means that while small earthquakes are more frequent, larger earthquakes occur less frequently but with potentially significant impact."
   ]
  },
  {
   "cell_type": "raw",
   "id": "85f21c06",
   "metadata": {},
   "source": [
    "ANSWER 3:-\n",
    "\n",
    "The five-number summary in statistics provides a concise summary of the distribution of a dataset. It consists of the following five values:\n",
    "\n",
    "Minimum: The smallest value in the dataset.\n",
    "First Quartile (Q1): The value below which 25% of the data falls.\n",
    "Median (Q2): The middle value of the dataset when it is sorted in ascending order. It divides the dataset into two equal halves.\n",
    "Third Quartile (Q3): The value below which 75% of the data falls.\n",
    "Maximum: The largest value in the dataset.\n",
    "The five-number summary is useful for understanding the spread, central tendency, and shape of the data. It helps in identifying outliers and comparing different datasets.\n",
    "\n",
    "For example, consider the dataset: 3, 7, 8, 9, 10, 12, 15, 18, 20, 25.\n",
    "\n",
    "The five-number summary would be:\n",
    "\n",
    "Minimum: 3\n",
    "Q1: 7.5 (average of 7 and 8)\n",
    "Median: 10\n",
    "Q3: 16.5 (average of 15 and 18)\n",
    "Maximum: 25 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecfc0689",
   "metadata": {},
   "source": [
    "ANSWER NO 4:-\n",
    "    \n",
    "pip install numpy pandas matplotlib\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Creating sample dataset\n",
    "data = {\n",
    "    'X': [1, 2, 3, 4, 5],\n",
    "    'Y': [2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "# Creating DataFrame from the dataset\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculating correlation coefficient\n",
    "correlation_coefficient = df['X'].corr(df['Y'])\n",
    "\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
    "\n",
    "# Plotting the dataset\n",
    "plt.scatter(df['X'], df['Y'], color='blue')\n",
    "plt.title('Scatter plot of X and Y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4de7a",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fec0a0b9",
   "metadata": {},
   "source": [
    "ANSWER NO:- 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1b7401f",
   "metadata": {},
   "source": [
    "ANSWER NO:- 2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7655d51a",
   "metadata": {},
   "source": [
    "ANSWER NO:- 3\n",
    "\n",
    "Let's start by implementing the steps for training and fine-tuning a Decision Tree using the wine dataset:\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Step 1: Load the wine dataset\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "y = wine_data.target\n",
    "\n",
    "# Step 2: Split the dataset into train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Hyperparameter tuning using RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'max_depth': randint(1, 10),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(dt_classifier, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "best_dt_model = random_search.best_estimator_\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Decision Tree:\", accuracy)\n",
    "\n",
    "Now, let's proceed with growing a random forest:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Step 1: Create 10 subsets of the training dataset\n",
    "n_trees = 10\n",
    "shuffle_split = ShuffleSplit(n_splits=n_trees, train_size=0.8, random_state=42)\n",
    "\n",
    "# Step 2: Train 1 decision tree on each subset\n",
    "trees = []\n",
    "for train_index, _ in shuffle_split.split(X_train):\n",
    "    clone_dt = clone(best_dt_model)\n",
    "    clone_dt.fit(X_train[train_index], y_train[train_index])\n",
    "    trees.append(clone_dt)\n",
    "\n",
    "# Step 3: Evaluate all the trees on the test dataset\n",
    "ensemble_predictions = []\n",
    "for tree in trees:\n",
    "    y_pred_tree = tree.predict(X_test)\n",
    "    ensemble_predictions.append(y_pred_tree)\n",
    "\n",
    "# Compute the ensemble prediction (majority voting)\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "ensemble_predictions = np.transpose(ensemble_predictions)\n",
    "ensemble_predictions = [np.argmax(np.bincount(pred)) for pred in ensemble_predictions]\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Accuracy of Random Forest:\", ensemble_accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5a772",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ca465fd",
   "metadata": {},
   "source": [
    "ANSWER NO 1:-\n",
    "\n",
    "(a). Implementing Deep Learning (DL) in a real-world application involves several steps:\n",
    "\n",
    "1. Define the Problem: Clearly define the problem you want to solve and determine if DL is the right approach. DL is suitable for tasks such as image and speech recognition, natural language processing, and more.\n",
    "\n",
    "2. Collect and Preprocess Data: Gather a sufficient amount of labeled data for training and testing. Preprocess the data to ensure it is in a suitable format and is representative of the real-world scenarios\n",
    "\n",
    "3. Choose a DL Framework: Select a deep learning framework such as TensorFlow, PyTorch, or Keras. These frameworks provide a set of tools and abstractions to simplify the implementation of neural networks.\n",
    "\n",
    "4. Design the Neural Network Architecture: Define the architecture of your neural network. This includes the number and type of layers, the activation functions, and the connections between neurons\n",
    "\n",
    "5. Train the Model: Split your dataset into training and testing sets. Train the model on the training set using an optimization algorithm, adjusting the weights and biases of the network to minimize the error\n",
    "\n",
    "6. Validate and Tune: Evaluate the model on the validation set to ensure it generalizes well to new data. Fine-tune hyperparameters and architecture based on performance\n",
    "\n",
    "7. Deploy the Model: Once satisfied with the model's performance, deploy it to the real-world environment. This could involve integrating it into a web application, a mobile app, or an embedded system.\n",
    "\n",
    "8. Monitor and Update: Regularly monitor the model's performance in the real-world environment. If necessary, update the model with new data and retrain it to adapt to changing conditions.\n",
    "(B). Use of Activation Function:\n",
    "\n",
    "Introducing Non-Linearity: Activation functions introduce non-linearities into the network, allowing it to model and understand complex patterns and relationships in the data.\n",
    "\n",
    "Learning Complex Representations: Non-linear activation functions enable the neural network to learn hierarchical and intricate representations of the input data, which is essential for capturing features at different levels of abstraction.\n",
    "\n",
    "Gradient Descent Optimization: Activation functions help in the optimization process during training by providing gradients that allow the network to adjust its parameters through backpropagation.\n",
    "\n",
    "(C). Problem Without Activation Function:- If neural networks had no activation functions, they would fail to learn the complex non-linear patterns that exist in real-world data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83f128dc",
   "metadata": {},
   "source": [
    "ANSWER NO 2:-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Design a simple ANN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "597cc7c4",
   "metadata": {},
   "source": [
    "ANSWER NO 3:-\n",
    "\n",
    "let's use the Boston Housing Prices dataset, which is a classic regression dataset available in scikit-learn.\n",
    "\n",
    "Here's:\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the Boston Housing Prices dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer with single neuron for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Mean Squared Error on Test Set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d77f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4143b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
